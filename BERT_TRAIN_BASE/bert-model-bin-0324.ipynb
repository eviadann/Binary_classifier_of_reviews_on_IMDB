{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ee8ec3-795d-4268-a451-95ad9a8c55e4",
   "metadata": {
    "editable": false
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9c5a88-d7e7-44fd-a41e-7caf568d3d4f",
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers datasets scikit-learn pandas openvino onnx nncf\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8542d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a1a4c10-312f-4218-b5a7-23ecf2862534",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T09:01:21.772226Z",
     "iopub.status.busy": "2025-04-01T09:01:21.771881Z",
     "iopub.status.idle": "2025-04-01T09:01:47.102608Z",
     "shell.execute_reply": "2025-04-01T09:01:47.101414Z",
     "shell.execute_reply.started": "2025-04-01T09:01:21.772200Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stepan/Documents/PythonProject/Binary_classifier_of_reviews_on_IMDB/venv_classifier/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f2c374e-69a9-476c-98de-3902e57323ad",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T12:25:23.556758Z",
     "iopub.status.busy": "2025-04-01T12:25:23.556255Z",
     "iopub.status.idle": "2025-04-01T12:25:23.561885Z",
     "shell.execute_reply": "2025-04-01T12:25:23.560925Z",
     "shell.execute_reply.started": "2025-04-01T12:25:23.556717Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a1d42-5170-4efe-ac31-98ba46b8ac87",
   "metadata": {
    "editable": false
   },
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f49f59f-9a57-4078-928b-516ae1d5fb7a",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T09:01:51.721712Z",
     "iopub.status.busy": "2025-04-01T09:01:51.721363Z",
     "iopub.status.idle": "2025-04-01T09:01:53.110144Z",
     "shell.execute_reply": "2025-04-01T09:01:53.109354Z",
     "shell.execute_reply.started": "2025-04-01T09:01:51.721678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# для проверки пайплайна\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_df = pd.DataFrame(dataset['train']).sample(10000, random_state=42).reset_index(drop=True)\n",
    "test_df = pd.DataFrame(dataset['test']).sample(2000, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feb6d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../datasets/train.csv', sep=';', index=False, encoding='utf-8')\n",
    "test_df.to_csv('../datasets/test.csv', sep=';', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2b6f800-e866-45da-9a4a-0c2718718346",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T09:01:54.416765Z",
     "iopub.status.busy": "2025-04-01T09:01:54.416383Z",
     "iopub.status.idle": "2025-04-01T09:01:54.423563Z",
     "shell.execute_reply": "2025-04-01T09:01:54.422418Z",
     "shell.execute_reply.started": "2025-04-01T09:01:54.416736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512, text_col=\"text\", label_col=\"label\"):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, self.text_col]\n",
    "        label = self.df.loc[idx, self.label_col]\n",
    "\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "  \n",
    "        item = {key: val.squeeze() for key, val in tokens.items()}\n",
    "        item[self.label_col] = torch.tensor(label, dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b370954-8596-4ad5-9f13-be2dca68cba8",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T09:01:59.435043Z",
     "iopub.status.busy": "2025-04-01T09:01:59.434744Z",
     "iopub.status.idle": "2025-04-01T09:01:59.440430Z",
     "shell.execute_reply": "2025-04-01T09:01:59.439165Z",
     "shell.execute_reply.started": "2025-04-01T09:01:59.435019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(df, tokenizer, text_column=\"text\", label_column=\"label\", max_length=512):\n",
    "    \n",
    "    dataset = Dataset.from_pandas(df[[text_column, label_column]])\n",
    "    \n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(\n",
    "            example[text_column], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=max_length\n",
    "        )\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns([text_column])\n",
    "    tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e089b530-0573-4875-ad78-2882652fb826",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T09:02:03.014249Z",
     "iopub.status.busy": "2025-04-01T09:02:03.013933Z",
     "iopub.status.idle": "2025-04-01T09:02:03.020981Z",
     "shell.execute_reply": "2025-04-01T09:02:03.019900Z",
     "shell.execute_reply.started": "2025-04-01T09:02:03.014226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_classification_report(model, tokenizer, df, text_column=\"text\", label_column=\"label\", max_length=512, batch_size=32):\n",
    "    predictions = []\n",
    "    labels = df[label_column].tolist()\n",
    "    \n",
    "    for i in tqdm(range(0, len(df), batch_size)):\n",
    "        batch_texts = df[text_column].iloc[i:i + batch_size].tolist()\n",
    "        \n",
    "        tokens = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "            batch_predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            predictions.extend(batch_predictions)\n",
    "        del tokens\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    report = classification_report(labels, predictions, target_names=[\"Class 0\", \"Class 1\"])\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12c5738-7461-4653-a081-7d18c3b17f9c",
   "metadata": {
    "editable": false
   },
   "source": [
    "==============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab2f675a-c0b7-4b6c-941e-26169b8feaa3",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T09:02:10.623097Z",
     "iopub.status.busy": "2025-04-01T09:02:10.622777Z",
     "iopub.status.idle": "2025-04-01T09:02:10.628020Z",
     "shell.execute_reply": "2025-04-01T09:02:10.627063Z",
     "shell.execute_reply.started": "2025-04-01T09:02:10.623072Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    # accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average=\"weighted\")\n",
    "    recall = recall_score(labels, predictions, average=\"weighted\")\n",
    "    \n",
    "    return {\"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db5cd50c-68be-4f0b-b0af-c4a91cd3b850",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T09:02:15.089717Z",
     "iopub.status.busy": "2025-04-01T09:02:15.089391Z",
     "iopub.status.idle": "2025-04-01T09:02:15.163562Z",
     "shell.execute_reply": "2025-04-01T09:02:15.162568Z",
     "shell.execute_reply.started": "2025-04-01T09:02:15.089692Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: cuda\n"
     ]
    }
   ],
   "source": [
    "device_cpu = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используемое устройство: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ed59a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобученная модель\n",
    "model_name = \"cointegrated/LaBSE-en-ru\"\n",
    "# model_name = \"distilbert/distilbert-base-multilingual-cased\"  # https://huggingface.co/distilbert/distilbert-base-multilingual-cased\n",
    "# model_name = \"cointegrated/rubert-tiny\"  # https://huggingface.co/cointegrated/rubert-tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40206f40-9770-47ba-91bd-1c2a2ab15c70",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T09:02:18.645329Z",
     "iopub.status.busy": "2025-04-01T09:02:18.644996Z",
     "iopub.status.idle": "2025-04-01T09:02:22.509887Z",
     "shell.execute_reply": "2025-04-01T09:02:22.508794Z",
     "shell.execute_reply.started": "2025-04-01T09:02:18.645278Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/LaBSE-en-ru and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78a08771-51d0-4448-96dc-c4204ef920bf",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T09:02:33.403053Z",
     "iopub.status.busy": "2025-04-01T09:02:33.402737Z",
     "iopub.status.idle": "2025-04-01T09:02:50.316960Z",
     "shell.execute_reply": "2025-04-01T09:02:50.315963Z",
     "shell.execute_reply.started": "2025-04-01T09:02:33.403013Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(generate_classification_report(model, tokenizer, test_df, text_column=\"text\", label_column=\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dd81c99-9953-421d-b313-b09e2e7dcfd6",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T09:02:50.318610Z",
     "iopub.status.busy": "2025-04-01T09:02:50.318236Z",
     "iopub.status.idle": "2025-04-01T09:02:50.322516Z",
     "shell.execute_reply": "2025-04-01T09:02:50.321686Z",
     "shell.execute_reply.started": "2025-04-01T09:02:50.318575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_df, tokenizer, max_length=512)\n",
    "test_dataset = TextDataset(test_df, tokenizer, max_length=512)\n",
    "# train_dataset = prepare_dataset(train_df, tokenizer, text_column=\"text\", label_column=\"label\", max_length=512)\n",
    "# test_dataset = prepare_dataset(test_df, tokenizer, text_column=\"text\", label_column=\"label\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34eb3afd-d986-4f54-8377-88f090071442",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T09:02:50.324479Z",
     "iopub.status.busy": "2025-04-01T09:02:50.324182Z",
     "iopub.status.idle": "2025-04-01T09:02:50.373423Z",
     "shell.execute_reply": "2025-04-01T09:02:50.372608Z",
     "shell.execute_reply.started": "2025-04-01T09:02:50.324456Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                      # Директория для сохранения результатов\n",
    "    eval_strategy=\"epoch\",                       # Оценка модели на каждой эпохе\n",
    "    learning_rate=2e-5,                          # Скорость обучения\n",
    "    per_device_train_batch_size=32,              # Размер батча для обучения\n",
    "    per_device_eval_batch_size=32,               # Размер батча для валидации\n",
    "    num_train_epochs=10,                         # Количество эпох\n",
    "    weight_decay=0.00001,                        # Коэффициент регуляризации\n",
    "    logging_dir='./logs',                        # Директория для логов (TensorBoard)\n",
    "    logging_steps=50,                            # Логирование каждые 500 шагов\n",
    "    load_best_model_at_end=True,                 # Загрузка лучшей модели по окончанию обучения\n",
    "    save_total_limit=2,                          # Сохраняем только 2 лучшие модели\n",
    "    metric_for_best_model=\"f1\",                  # Ключевая метрика для выбора лучшей модели\n",
    "    greater_is_better=True,                      # Лучшая модель — та, где метрика больше\n",
    "    save_strategy=\"epoch\",                       # Сохраняем модель на каждой эпохе\n",
    "    report_to=\"tensorboard\",                     # Используем TensorBoard для логирования\n",
    "    optim=\"adamw_torch\",                         # Явно указываем AdamW как оптимизатор\n",
    "    warmup_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40151f90-1d13-410f-867d-64e3cccae051",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T09:02:50.374574Z",
     "iopub.status.busy": "2025-04-01T09:02:50.374344Z",
     "iopub.status.idle": "2025-04-01T09:26:59.672258Z",
     "shell.execute_reply": "2025-04-01T09:26:59.671416Z",
     "shell.execute_reply.started": "2025-04-01T09:02:50.374554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,                                   # Тренировочный датасет\n",
    "#     eval_dataset=test_dataset,                                     # Валидационный датасет\n",
    "#     tokenizer=tokenizer,                                           # Токенизатор\n",
    "#     compute_metrics=compute_metrics,                               # Функция вычисления метрик\n",
    "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # Ранняя остановка (2 эпохи)\n",
    "#     # data_collator=data_collator\n",
    "# )\n",
    "\n",
    "# # Запуск тренировки\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd78d10d-6861-44a0-890d-050e76cec19a",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T09:29:12.496966Z",
     "iopub.status.busy": "2025-04-01T09:29:12.496585Z",
     "iopub.status.idle": "2025-04-01T09:29:14.442251Z",
     "shell.execute_reply": "2025-04-01T09:29:14.441428Z",
     "shell.execute_reply.started": "2025-04-01T09:29:12.496941Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель сохранена в: ../model/bert_sentiment_model\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../model/bert_sentiment_model\"\n",
    "\n",
    "# Сохранение модели\n",
    "# trainer.save_model(model_path)\n",
    "# # Сохранение токенизатора\n",
    "# tokenizer.save_pretrained(model_path)\n",
    "\n",
    "print(f\"Модель сохранена в: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947e0a9b",
   "metadata": {},
   "source": [
    "### Экспортируем модель в ONNX формат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48f34ebf-2d7f-4c95-a035-908e3b905430",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T13:43:39.531495Z",
     "iopub.status.busy": "2025-04-01T13:43:39.531081Z",
     "iopub.status.idle": "2025-04-01T13:43:39.714846Z",
     "shell.execute_reply": "2025-04-01T13:43:39.713880Z",
     "shell.execute_reply.started": "2025-04-01T13:43:39.531459Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Загружаем обученную модель\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "model = model.to(device_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c44b7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>\n",
      "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7709b1c-64ef-4522-9d08-f7f4cdc3be79",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T13:43:42.229207Z",
     "iopub.status.busy": "2025-04-01T13:43:42.228877Z",
     "iopub.status.idle": "2025-04-01T13:43:46.650677Z",
     "shell.execute_reply": "2025-04-01T13:43:46.649678Z",
     "shell.execute_reply.started": "2025-04-01T13:43:42.229178Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель сохранена в ../model/onnx/model.onnx\n"
     ]
    }
   ],
   "source": [
    "onnx_path = \"../model/onnx/model.onnx\"\n",
    "\n",
    "dummy_input = (\n",
    "    torch.randint(0, 100, (1, 512)),\n",
    "    torch.ones((1, 512), dtype=torch.int64)               \n",
    ")\n",
    "torch.onnx.export(\n",
    "    model, dummy_input, onnx_path, \n",
    "    input_names=[\"input_ids\", \"attention_mask\"], output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\"},\n",
    "        \"attention_mask\": {0: \"batch_size\"},\n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    },\n",
    "    opset_version=14\n",
    ")\n",
    "\n",
    "print(f\"Модель сохранена в {onnx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199cb568",
   "metadata": {},
   "source": [
    "### Экспотируем в OpenVINO и выполняем квантизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a724969a-26c6-484f-adf7-9cbbbccf81f3",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T13:59:16.383883Z",
     "iopub.status.busy": "2025-04-01T13:59:16.383538Z",
     "iopub.status.idle": "2025-04-01T13:59:16.388157Z",
     "shell.execute_reply": "2025-04-01T13:59:16.387208Z",
     "shell.execute_reply.started": "2025-04-01T13:59:16.383857Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "from openvino import convert_model\n",
    "import nncf\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_path = \"../model/model_openvino/model_openvino.xml\"  # Файл для сохранения квантизированной модели\n",
    "onnx_path = \"../model/onnx/model.onnx\"\n",
    "openvino_model_path = \"../model/model_openvino/model_openvino.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee93d4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226d06f-58e1-49f1-b0d6-b868e8b4df38",
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Проверка модели и вывод структуры\n",
    "onnx.checker.check_model(onnx_path)\n",
    "print(onnx.helper.printable_graph(onnx_path.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ac99b6f-9c54-40c6-8753-9a449d3b9ce2",
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-04-01T13:59:20.809082Z",
     "iopub.status.busy": "2025-04-01T13:59:20.808792Z",
     "iopub.status.idle": "2025-04-01T13:59:21.207822Z",
     "shell.execute_reply": "2025-04-01T13:59:21.206556Z",
     "shell.execute_reply.started": "2025-04-01T13:59:20.809060Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель сохранена в ../model/model_openvino/model_openvino.xml\n"
     ]
    }
   ],
   "source": [
    "model_ir = convert_model(onnx_path)\n",
    "ov.serialize(model_ir, openvino_model_path)\n",
    "print(f\"Модель сохранена в {openvino_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3577b7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель загружена!\n"
     ]
    }
   ],
   "source": [
    "# Загружаем модель OpenVINO\n",
    "core = ov.runtime.Core()\n",
    "model_ov = core.read_model(openvino_model_path)\n",
    "print(\"Модель загружена!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7c21280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input name: input_ids, shape: [?,512]\n",
      "Input name: attention_mask, shape: [?,512]\n"
     ]
    }
   ],
   "source": [
    "# import openvino.runtime as ov\n",
    "\n",
    "# core = ov.Core()\n",
    "model_ov = core.read_model(openvino_model_path)\n",
    "\n",
    "for input_node in model_ov.inputs:\n",
    "    print(f\"Input name: {input_node.get_any_name()}, shape: {input_node.get_partial_shape()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c439c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем уже загруженный tokenizer\n",
    "calibration_size = 500\n",
    "calibration_df = train_df.sample(calibration_size, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16fa52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем DataLoader\n",
    "calibration_loader = torch.utils.data.DataLoader(\n",
    "    TextDataset(calibration_df, tokenizer, max_length=512), \n",
    "    batch_size=32, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e5cb2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция трансформации для калибровки\n",
    "def transform_fn(data_item):\n",
    "    input_ids = data_item[\"input_ids\"].numpy()\n",
    "    attention_mask = data_item[\"attention_mask\"].numpy()\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af6b0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание калибровочного датасета для NNCF\n",
    "calibration_dataset = nncf.Dataset(calibration_loader, transform_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ddfbb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "(32, 512)\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(calibration_dataset.get_inference_data()))\n",
    "print(sample.keys())  # Должен содержать 'input'\n",
    "print(sample['input_ids'].shape)  # Должно быть (batch_size, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = nncf.quantize(\n",
    "    model_ov, \n",
    "    calibration_dataset, \n",
    "    model_type=nncf.ModelType.TRANSFORMER, \n",
    "    target_device=nncf.TargetDevice.CPU,\n",
    "    fast_bias_correction=True,\n",
    "    preset=nncf.quantization.QuantizationPreset.PERFORMANCE,\n",
    "    advanced_parameters=nncf.quantization.advanced_parameters.AdvancedQuantizationParameters(\n",
    "        batchwise_statistics=False\n",
    "    )    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov.serialize(quantized_model, \n",
    "             \"../model/quantized_model/quantized_model.xml\", \n",
    "             \"../model/quantized_model/quantized_model.bin\")\n",
    "print(\"Квантизация завершена.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd71045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Типы операций в модели: {'Divide', 'Reshape', 'FakeQuantize', 'Gather', 'Tanh', 'ShapeOf', 'Unsqueeze', 'Gelu', 'Select', 'Result', 'Add', 'Transpose', 'Multiply', 'Concat', 'Slice', 'MVN', 'Parameter', 'Convert', 'Constant', 'MatMul', 'Subtract', 'Broadcast', 'Sqrt', 'Equal', 'Softmax'}\n",
      "Похоже, модель содержит INT8-операции.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/importlib/util.py:247: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  self.__spec__.loader.exec_module(self)\n"
     ]
    }
   ],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "core = Core()\n",
    "model = core.read_model(\"../model/bert_quantized_model/quantized_model.xml\")\n",
    "\n",
    "# Проверка, какие типы операций использует модель\n",
    "op_types = set(op.get_type_name() for op in model.get_ops())\n",
    "print(\"Типы операций в модели:\", op_types)\n",
    "\n",
    "# Проверка, есть ли INT8-операции\n",
    "if any(\"FakeQuantize\" in op or \"Quantize\" in op or \"Dequantize\" in op or \"Convolution\" in op for op in op_types):\n",
    "    print(\"Похоже, модель содержит INT8-операции.\")\n",
    "else:\n",
    "    print(\"В модели не обнаружено признаков INT8-квантизации.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8717191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INT8-связанные операции в модели:\n",
      "\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'char'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "MatMul          | Выходной тип: [<Type: 'float32'>]\n",
      "Convert         | Выходной тип: [<Type: 'float32'>]\n",
      "FakeQuantize    | Выходной тип: [<Type: 'float32'>]\n",
      "\n",
      "Квантизация, похоже, успешно применена.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/importlib/util.py:247: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  self.__spec__.loader.exec_module(self)\n"
     ]
    }
   ],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "# Путь к квантизованной модели\n",
    "model_path = \"../model/quantized_model/quantized_model.xml\"\n",
    "\n",
    "# Загружаем модель\n",
    "core = Core()\n",
    "model = core.read_model(model_path)\n",
    "\n",
    "# Получаем все операции модели\n",
    "ops = model.get_ops()\n",
    "\n",
    "# Список операций, связанных с квантизацией\n",
    "int8_related_ops = [\"FakeQuantize\", \"Quantize\", \"Dequantize\", \"Convert\", \"Convolution\", \"MatMul\"]\n",
    "\n",
    "print(\"\\nINT8-связанные операции в модели:\\n\")\n",
    "\n",
    "found = False\n",
    "for op in ops:\n",
    "    op_type = op.get_type_name()\n",
    "    if op_type in int8_related_ops:\n",
    "        found = True\n",
    "        print(f\"{op_type:15} | Выходной тип: {[out.get_element_type() for out in op.outputs()]}\")\n",
    "\n",
    "if not found:\n",
    "    print(\"Не найдено INT8-операций. Возможно, квантизация не была применена.\")\n",
    "else:\n",
    "    print(\"\\nКвантизация, похоже, успешно применена.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f42731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "\n",
    "# Пути к моделям\n",
    "fp32_path = \"../model/model_openvino/model_openvino.xml\"\n",
    "int8_path = \"../model/bert_quantized_model/quantized_model.xml\"\n",
    "\n",
    "# Создаём runtime\n",
    "core = Core()\n",
    "fp32_model = core.compile_model(core.read_model(fp32_path), \"CPU\")\n",
    "int8_model = core.compile_model(core.read_model(int8_path), \"CPU\")\n",
    "\n",
    "# Имя входного тензора\n",
    "input_key = next(iter(fp32_model.inputs))\n",
    "\n",
    "# Получаем нужную форму входа из модели\n",
    "# input_shape = fp32_model.input(0).shape\n",
    "# seq_len = input_shape[1]  # обычно 512\n",
    "seq_len = 512\n",
    "\n",
    "# Пример входных данных (имитируем батч из 1 примера нужной длины)\n",
    "dummy_input = np.random.randint(0, 30522, size=(1, seq_len)).astype(\"int32\")\n",
    "\n",
    "print(f\"Generated dummy input shape: {dummy_input.shape}\")\n",
    "\n",
    "# Пример входных данных (имитируем 1 батч токенов, длиной 128)\n",
    "# dummy_input = np.random.randint(0, 30522, size=(1, 128)).astype(\"int32\")\n",
    "\n",
    "# Прогрев\n",
    "_ = fp32_model({input_key: dummy_input})\n",
    "_ = int8_model({input_key: dummy_input})\n",
    "\n",
    "# Функция замера\n",
    "def measure_latency(model, name, runs=100):\n",
    "    start = time.time()\n",
    "    for _ in range(runs):\n",
    "        model({input_key: dummy_input})\n",
    "    end = time.time()\n",
    "    avg_latency = (end - start) / runs * 1000  # в мс\n",
    "    print(f\"{name} latency: {avg_latency:.2f} ms\")\n",
    "\n",
    "# Сравнение\n",
    "measure_latency(fp32_model, \"FP32\")\n",
    "measure_latency(int8_model, \"INT8\")\n",
    "\n",
    "# Предсказания (1 пример)\n",
    "fp32_result = fp32_model({input_key: dummy_input})\n",
    "int8_result = int8_model({input_key: dummy_input})\n",
    "\n",
    "# Сравнение выходов\n",
    "fp32_output = list(fp32_result.values())[0]\n",
    "int8_output = list(int8_result.values())[0]\n",
    "diff = np.mean(np.abs(fp32_output - int8_output))\n",
    "\n",
    "print(f\"\\nСреднее абсолютное отличие между FP32 и INT8: {diff:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a65ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Инициализация ===\n",
    "core = Core()\n",
    "fp32_model = core.compile_model(\"../model/model_openvino/model_openvino.xml\", \"CPU\")\n",
    "int8_model = core.compile_model(\"../model/quantized_model/quantized_model.xml\", \"CPU\")\n",
    "\n",
    "# Названия входов и выходов\n",
    "input_keys = [inp.get_any_name() for inp in fp32_model.inputs]\n",
    "output_key = fp32_model.outputs[0].get_any_name()\n",
    "\n",
    "# === Функция для предсказания на OpenVINO модели ===\n",
    "def predict_ov(model, tokenizer, df, max_length=512, batch_size=32):\n",
    "    predictions = []\n",
    "    for i in tqdm(range(0, len(df), batch_size)):\n",
    "        texts = df[\"text\"].iloc[i:i+batch_size].tolist()\n",
    "        tokens = tokenizer(texts, padding=\"max_length\", truncation=True,\n",
    "                           max_length=max_length, return_tensors=\"np\")\n",
    "\n",
    "        # Предсказания\n",
    "        inputs = {k: tokens[k] for k in input_keys if k in tokens}\n",
    "        logits = model(inputs)[output_key]\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        predictions.extend(preds)\n",
    "    return predictions\n",
    "\n",
    "# === Получение предсказаний ===\n",
    "true_labels = test_df[\"label\"].tolist()\n",
    "preds_fp32 = predict_ov(fp32_model, tokenizer, test_df)\n",
    "preds_int8 = predict_ov(int8_model, tokenizer, test_df)\n",
    "\n",
    "# === Сравнение метрик ===\n",
    "print(\"FP32:\")\n",
    "print(classification_report(true_labels, preds_fp32, target_names=[\"Class 0\", \"Class 1\"]))\n",
    "print(\"INT8:\")\n",
    "print(classification_report(true_labels, preds_int8, target_names=[\"Class 0\", \"Class 1\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72ea22a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FP32: 100%|██████████| 2/2 [00:29<00:00, 14.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FP32 (реальные данные):\n",
      "Среднее время: 7273.31 мс\n",
      "Среднее использование RAM: 3328.04 MB\n",
      "Загрузска CPU: 19.30%\n",
      "Загрузска CPU: 21.60%\n",
      "Загрузска CPU: 25.10%\n",
      "Загрузска CPU: 26.40%\n",
      "Загрузска CPU: 98.55%\n",
      "Загрузска CPU: 97.80%\n",
      "Загрузска CPU: 99.10%\n",
      "Загрузска CPU: 98.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INT8: 100%|██████████| 2/2 [00:21<00:00, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INT8 (реальные данные):\n",
      "Среднее время: 5252.95 мс\n",
      "Среднее использование RAM: 5115.29 MB\n",
      "Загрузска CPU: 22.75%\n",
      "Загрузска CPU: 25.70%\n",
      "Загрузска CPU: 29.10%\n",
      "Загрузска CPU: 30.70%\n",
      "Загрузска CPU: 98.10%\n",
      "Загрузска CPU: 98.30%\n",
      "Загрузска CPU: 98.45%\n",
      "Загрузска CPU: 97.95%\n",
      "\n",
      "Результаты сохранены в resource_usage_real_input.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from openvino.runtime import Core\n",
    "\n",
    "# === Настройки ===\n",
    "model_path_fp32 = \"../model/model_openvino/model_openvino.xml\"\n",
    "model_path_int8 = \"../model/quantized_model/quantized_model.xml\"\n",
    "model_name = \"cointegrated/LaBSE-en-ru\"\n",
    "csv_file = \"resource_usage_real_input.csv\"\n",
    "batch_size = 16\n",
    "max_length = 512\n",
    "n_examples = 32\n",
    "\n",
    "# === Загрузка моделей ===\n",
    "core = Core()\n",
    "fp32_model = core.compile_model(model_path_fp32, \"CPU\")\n",
    "int8_model = core.compile_model(model_path_int8, \"CPU\")\n",
    "input_keys = [inp.get_any_name() for inp in fp32_model.inputs]\n",
    "output_key = fp32_model.outputs[0].get_any_name()\n",
    "\n",
    "# === Загрузка токенизатора и данных ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "from datasets import load_dataset\n",
    "test_df = pd.DataFrame(load_dataset(\"imdb\")[\"test\"]).sample(n_examples, random_state=42)\n",
    "\n",
    "# === Токенизация всего батча ===\n",
    "tokens = tokenizer(\n",
    "    test_df[\"text\"].tolist(),\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=max_length,\n",
    "    return_tensors=\"np\"\n",
    ")\n",
    "\n",
    "# === Разбивка на батчи ===\n",
    "def get_batches(tokens, batch_size):\n",
    "    total = len(tokens[\"input_ids\"])\n",
    "    for i in range(0, total, batch_size):\n",
    "        yield {k: v[i:i+batch_size] for k, v in tokens.items()}\n",
    "\n",
    "# === CSV заголовки ===\n",
    "# with open(csv_file, mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow([\"Model\", \"BatchSize\", \"AvgLatency_ms\", \"AvgRAM_MB\", \"AvgCPU_Percent\"])\n",
    "\n",
    "# === Измерение ===\n",
    "def measure_with_real_data(model, name):\n",
    "    all_latencies = []\n",
    "    all_cpu = []\n",
    "    all_mem = []\n",
    "    psutil.cpu_percent(interval=None, percpu=True)\n",
    "\n",
    "    for batch in tqdm(list(get_batches(tokens, batch_size)), desc=f\"{name}\"):\n",
    "        filtered_batch = {k: v for k, v in batch.items() if k in input_keys}\n",
    "        # Прогрев\n",
    "        _ = model(filtered_batch)\n",
    "\n",
    "        start_time = time.time()\n",
    "        _ = model(filtered_batch)\n",
    "        end_time = time.time()\n",
    "\n",
    "        latency = (end_time - start_time) * 1000  # ms\n",
    "        cpu = psutil.cpu_percent(interval=None, percpu=True)\n",
    "        mem = psutil.Process().memory_info().rss / (1024 ** 2)\n",
    "\n",
    "        all_latencies.append(latency)\n",
    "        all_cpu.append(cpu)\n",
    "        all_mem.append(mem)\n",
    "\n",
    "    avg_latency = np.mean(all_latencies)\n",
    "    avg_ram = np.mean(all_mem)\n",
    "    # avg_cpu = np.mean(all_cpu)\n",
    "    num_cpu = psutil.cpu_count()\n",
    "    len_lst_all_cpu = len(all_cpu)\n",
    "    avg_cpu = np.zeros(num_cpu)\n",
    "    for i in range(len_lst_all_cpu):\n",
    "        for j in range(num_cpu):\n",
    "            avg_cpu[j] += all_cpu[i][j]\n",
    "    for i in range(num_cpu):\n",
    "        avg_cpu[i] = avg_cpu[i] / len_lst_all_cpu\n",
    "\n",
    "    print(f\"\\n{name} (реальные данные):\")\n",
    "    # print(all_latencies)\n",
    "    # for i in all_latencies:\n",
    "    #     print(f\"Время батча {i + 1}: {all_latencies[i]} мс\")\n",
    "    print(f\"Среднее время: {avg_latency:.2f} мс\")\n",
    "    # print(all_mem)\n",
    "    # for i in all_mem:\n",
    "    #     print(f\"Использование RAM для батча {i + 1}: {all_mem[i]} мс\")\n",
    "    print(f\"Среднее использование RAM: {avg_ram:.2f} MB\")\n",
    "    for i in avg_cpu:\n",
    "        print(f\"Загрузска CPU: {i:.2f}%\")\n",
    "    # print(avg_cpu)\n",
    "    # for i in all_cpu:\n",
    "    #     print(f\"Загрузска CPU для батча {i + 1}: {all_cpu[i]} мс\")\n",
    "    # print(f\"Средняя загрузка CPU: {avg_cpu:.2f}%\")\n",
    "\n",
    "\n",
    "    # with open(csv_file, mode='a', newline='') as file:\n",
    "    #     writer = csv.writer(file)\n",
    "    #     writer.writerow([name, batch_size, avg_latency, avg_ram, avg_cpu])\n",
    "\n",
    "# === Сравнение ===\n",
    "measure_with_real_data(fp32_model, \"FP32\")\n",
    "measure_with_real_data(int8_model, \"INT8\")\n",
    "print(f\"\\nРезультаты сохранены в {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "18dc3623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scputimes(user=11245.06, nice=19.63, system=1437.43, idle=202469.65, iowait=219.24, irq=0.0, softirq=34.18, steal=0.0, guest=0.0, guest_nice=0.0)\n",
      "[10.2, 10.2, 32.0, 10.0, 13.7, 8.2, 7.8, 7.8]\n",
      "scputimes(user=7.9, nice=0.0, system=1.6, idle=90.2, iowait=0.2, irq=0.0, softirq=0.0, steal=0.0, guest=0.0, guest_nice=0.0)\n",
      "8\n",
      "scpustats(ctx_switches=62233430, interrupts=64095585, soft_interrupts=25131366, syscalls=0)\n",
      "scpufreq(current=1286.3487499999999, min=800.0, max=4100.0)\n",
      "(1.14111328125, 1.9072265625, 2.30126953125)\n",
      "1744785058.0\n"
     ]
    }
   ],
   "source": [
    "print(psutil.cpu_times())\n",
    "print(psutil.cpu_percent(interval=0.5, percpu=True))\n",
    "print(psutil.cpu_times_percent())\n",
    "print(psutil.cpu_count())\n",
    "print(psutil.cpu_stats())\n",
    "print(psutil.cpu_freq())\n",
    "print(psutil.getloadavg())\n",
    "print(psutil.boot_time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79baa35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 5.]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2655743,
     "sourceId": 4548721,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv_classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
